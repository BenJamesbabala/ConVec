# ConVec: Vector Embedding of Wikipedia Concepts and Entities

WikipediaParser folder contains the code to extract and prepare the Wikipedia dump.

Please find in the following, link to pre-traind Concept, Word and Entitie vectors (as a result of this project): 
- The Wikipedia ID to Title map file (This file maps the Wikipedia ID of a page to its title): https://web.cs.dal.ca/~sherkat/Files/ID_title_map.zip (210MB)
or https://dalu-my.sharepoint.com/personal/eh379022_dal_ca/_layouts/15/guestaccess.aspx?docid=069df6faaa6c744a7b194c6522ed168c7&authkey=Af1PzeeutGuzkr-SGJIR_z4
- Traind Concepts, Entities and Words: Concepts and entities are presented by their Wikipedia ID.
 - ConVec https://web.cs.dal.ca/~sherkat/Files/WikipediaClean5Negative300Skip10.zip (3.3GB) 
 - ConVec Fine Tuned (6.86GB)
 - ConVec Heuristic (3.75GB)
 - ConVec Only Anchors (3.57GB)

Please cite to the following paper if you used the code, datasets and vector embedings:
```
@misc{arXiv:1702.03470,
   Author = {Ehsan Sherkat, Evangelos Milios},
   Title  = {Vector Embedding of Wikipedia Concepts and Entities},
   Year   = {2017},
   Url    = {https://arxiv.org/abs/1702.03470}
   Eprint = {arXiv:1702.03470},
}
```
